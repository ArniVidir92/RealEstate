## Residuals
Begin by looking at the residuals from this model

```{r echo=FALSE, message=FALSE, error=FALSE, fig.cap="Indexplot for the residuals."}
indexPlotResiduals(lm.all)
```

Here the blue and green line represent 2 and 3 standard deviations from the mean. We identify those points that are two standard deviations away from the mean. We clearly see that there are some possible outliers that need further diagnostics.

## Leverages

The next thing to do is looking at the leverages, that is the measure of how far independent variable values of an observation are from those of the other observation. Figure two marks those points that are more than $\frac{2p}{n}=\frac{2\cdot `r length(summary(lm.all)$coef[,1])`}{`r length(train$nuvirdi)`}\approx 0.05$

```{r echo=FALSE, message=FALSE, error=FALSE, fig.cap="Indexplot of leverages"}
indexPlotLeverage(lm.all)
```


## Studentized residuals

Studentized residuals are sometimes preferred in residual plots as they have been standardized to have equal variance. They are also a big part in the Jackkiife residulas that follows

```{r echo=FALSE, message=FALSE, error=FALSE, fig.cap="Index plot of the studentized residuals."}
stres <- indexPlotStResiduals(lm.all)+ylim(-5,20)
```

Blue and green line represent as before 2 and 3 sd from the mean. 

```{r echo=FALSE, message=FALSE, error=FALSE, fig.cap="Index plot of the jackknive residuals"}
jackres <- indexPlotJackResiduals(lm.all)+ylim(-5,20)
```
 
```{r echo=FALSE, message=FALSE, error=FALSE}
multiplot(stres,jackres,cols = 2)
```

Cook's distance (calculated w.r.t Jackknife and Std.Residuals) is a good way to diagnose influential points in the model. Points with high Cooks distance are affecting the model more than the others. The green points have high Cooks distance, but the blue points have high Cooks distance but also high leverage.

```{r echo=FALSE, message=FALSE, error=FALSE}
indexPlotCookdistance(lm.all)
```

To see how well the model fits the data, we plot the fitted value against residuals. This should be scatterplot with no specified form.

```{r echo=FALSE, message=FALSE, error=FALSE}
fitVsres <- fittedVsresiduals(lm.all)
```

We clearly see this is not what we expected to see. Also the QQ plot This means we have to do some transformation and remove the biggest outliers.  

  
```{r echo=FALSE, message=FALSE, error=FALSE}
QQ <- QQplotResiduals(lm.all) + labs(title="QQ plot of train data")
```

```{r echo=FALSE, message=FALSE, error=FALSE}
multiplot(fitVsres,QQ,cols = 2)
```


```{r echo=FALSE, message=FALSE, error=FALSE}
###################################### R squared adjusted fyrir lm.all
Radj.all <- CalculateRadjusted(lm.all, test) # 0.7990392
```


First we calculate the R-adjusted for the first model and the whole train data and get $R-adjusted$= `r Radj.all`. The R-squared for this data set and model is, $R-squared$=`r summary(lm.all)$r.squared`
Now by removing the residuals that have std. residuals $>$ 3 we have new model.

```{r echo=FALSE, message=FALSE, error=FALSE}
## Tek út punkta með Std. Res > 3
trainNO <- removeOutliersWStdResMoreThanThree(train)
lm.allNoOutlier <- lm(nuvirdi ~ . ,data = trainNO)
sumNO <- summary(lm.allNoOutlier)
Radj.allNoOutliers <- CalculateRadjusted(lm.allNoOutlier, test) # 0.7328961 # Ottó: 0.7905836
```



The plots below show that the model in instatnly better for our train data, just by removing some outliers.

```{r echo=FALSE, message=FALSE, error=FALSE}
##QQ plot af nýju modeli án Std. Resiudals

fittedNo <- fittedVsresiduals(lm.allNoOutlier)
QQno <- QQplotResiduals(lm.allNoOutlier) + labs(title="QQ plot no Std res > 3")
multiplot(fittedNo,QQno,cols = 2)
#Radj.allNoOutliers
```

The R-adjusted for fitting the model with new train data is $R-adjusted No outliers$=`r Radj.allNoOutliers` while R-squared for the train data gets better, $R-squared$=`r sumNO$r.squared`

With the Cooks distance we can find the most influential points affecting our model. We want to remove all influential points with Cooks distance $>$ `r 4/n` and see how to model fits to that data.


```{r echo=FALSE, message=FALSE, error=FALSE}

## Tek út Influential punkta með Cooks Distnace =4/n

trainNoInflu <- removeInfluential(train,maxCookDistance = 0.05)
lm.allNoInfluential <- lm(nuvirdi ~ . ,data = trainNoInflu)
sumNI <- summary(lm.allNoInfluential)
#sumNI$r.squared
Radj.allNoInfluential <- CalculateRadjusted(lm.allNoInfluential, test)# 0.7327364 # 0.7888572
#Radj.allNoInfluential
```


```{r echo=FALSE, message=FALSE, error=FALSE}
## QQ plot and Fitted of No Influential

fittedNoInf <- fittedVsresiduals(lm.allNoInfluential)
QQnoInf <- QQplotResiduals(lm.allNoInfluential) + labs(title="QQ plot no influential")
multiplot(fittedNoInf,QQnoInf,cols = 2)
```

Now our R-adjusted is still worse than for the whole train data, $R-adjusted$= `r Radj.allNoInfluential` while R-squared keeps getting higher $R-squared$=`r sumNI$r.squared`

Last data set we make is with no influential points and no outliers. The previous model should fit this data set very well but on the other half R-adjuested might be getting lower.

```{r echo=FALSE, message=FALSE, error=FALSE}
#No influential and no residuals
trainNONI <- removeOutliersWStdResMoreThanThree(trainNoInflu)
lm.allNoInflueNoOutlier <- lm(nuvirdi ~ . ,data = trainNONI)
#sumNONI <- summary(lm.allNoInfluential)
#sumNI$r.squared
Radj.allNONI <- CalculateRadjusted(lm.allNoInflueNoOutlier, test)
#trainNoInfl <- removeInfluential(train,maxCookDistance = 4/n)

##QQ plot of no influentiial points and 

fittedNoInfNoOut <- fittedVsresiduals(lm.allNoInflueNoOutlier)
QQNoInfNoOut <- QQplotResiduals(lm.allNoInflueNoOutlier) + labs(title="QQ plot no inf no std res > 3")
multiplot(fittedNoInfNoOut,QQNoInfNoOut,cols = 2)

Radj.allNoInfluentialNoOutlier <- CalculateRadjusted(lm.allNoInflueNoOutlier, test)
Radj.allNoInfluentialNoOutlier
```

```{r}

```

